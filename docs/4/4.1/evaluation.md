# 模型评估方法与准则

- 编辑：李竹楠
- 日期：2024/02/04

在AI场景下我们需要定量的数值化指标来指导我们更好地应用模型对数据进行学习和建模。事实上，在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文的结构如下：

1. 首先介绍模型**评估的目标**，说明模型需要达到什么样的标准，以及为了达到这样的标准需要做些什么工作。
2. 介绍两种不同的**实验方法**，分别是离线实验和在线实验
3. 然后，在评估模型之前，需要对数据集进行**验证**，并且合理**划分数据集**，目的是保证我们后续计算得到的评估指标是可靠有效的。
4. 针对于**回归任务**，介绍评估的方法和准则。
5. 针对于**分类任务**，介绍评估的方法和准则。

## 1. 模型评估的目标

**模型评估的目标是选出泛化能力强的模型完成机器学习任务**。实际的机器学习任务往往需要进行大量的实验，经过反复调参、使用多种模型算法（甚至多模型融合策略）来完成自己的机器学习问题，并观察哪种模型算法在什么样的参数下能够最好地完成任务。

**泛化能力强**的模型能很好地适用于未知的样本，模型的错误率低、精度高。机器学习任务中，我们希望最终能得到**准确预测未知标签的样本、泛化能力强的模型**。但是我们无法提前获取“未知的样本”，因此我们会基于已有的数据进行切分来完成模型训练和评估，借助于切分出的数据进行评估，可以很好地判定模型状态（过拟合或欠拟合），进而迭代优化。

在建模过程中，为了获得泛化能力强的模型，我们需要一整套方法及评价指标。

- **评估方法**：为保证客观地评估模型，对数据集进行的有效划分实验方法。
- **性能指标**：量化地度量模型效果的指标。

## 2. 实验方法

分为**离线实验方法**和**在线实验方法**。

**模型评估通常指离线试验**。原型设计（Prototyping）阶段及离线试验方法，包含以下几个过程：

1. 使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型。
2. 对模型进行验证（Validation）与离线评估（Offline Evaluation）。
3. 通过评估指标选择一个较好的模型。

除了离线评估之外，其实还有一种在线评估的实验方法。由于模型是在老的模型产生的数据上学习和验证的，而**线上**的数据与之前是不同的，因此离线评估并不完全代表线上的模型结果。因此我们需要在线评估，来验证模型的有效性。

**A/B TEST 是目前在线测试中最主要的方法**。A/B TEST 是为同一个目标制定两个方案，让一部分用户使用A方案，让另一部分用户使用B方案，记录下用户的使用情况，看哪个方案更符合设计目标。如果不做AB实验直接上线新方案，新方案甚至可能会毁掉你的产品。

在**离线评估**中，经常使用准确率（Accuracy）、查准率（Precision）、召回率（Recall）、ROC、AUC、PRC等指标来评估模型。

**在线评估**与离线评估所用的评价指标不同，一般使用一些商业评价指标，如用户生命周期值（Customer Lifetime value）、广告点击率（Click Through Rate）、用户流失率（Customer Churn Rate）等标。

## 3. 常见的模型验证方法

下面我们来了解一下模型验证方法，主要涉及到对**完整数据集不同的有效划分方法**，保证我们后续计算得到的评估指标是可靠有效的，进而进行模型选择和优化（模型验证时防止**数据泄漏**非常重要）。

> 机器学习中的数据泄露？
> 
> 数据泄露在预测模型中是一个非常大的问题。数据泄露是当**来自于训练集外部的信息**被用于创建模型。下面要说明在预测模型中数据泄露的一下几个问题：
> 
> - 什么是数据泄露？ 
> - 数据泄露的迹象，为什么数据泄露是个问题？
> - 关于改善数据泄露问题的技巧。
> 
> 什么是数据泄露？ 
> 
> 数据泄露造成了“过分乐观(overly optimistic)”的模型，即使这些模型不是完全有效的。数据泄漏是指使用**训练数据集外部的信息**来创建模型。这些**额外**的信息可以让模型学习或知道一些它**本来不知道**的东西，从而使所构建的模型的估计性能失效。
>
> 为什么数据泄露是一个问题？
>
> 这里有三个原因：
> - 如果在竞赛中，模型将使用泄漏数据，而不是对潜在问题的良好通用模型。
> - 如果在公司提供的数据中，撤销匿名和混淆可能会导致您意想不到的隐私泄露。
> - 如果将模型部署在生产环境中，这些模型实际上是无用的，不能在生产中使用。
> 
> 如何检测是否产生了数据泄露？
>
> 一个简单的方法就是，是否实现了一个性能好的令人难以置信的模型，就像实现了一个高准确率的博彩预测。
>
> 一些关于如何避免数据泄露的技巧： 
> 
> - 在交叉验证中执行数据准备(data preparation)
> - 保留一个验证数据集，以便对开发的模型进行最终的全面检查。
>
> 通常来说，同时使用这两种方法是一个好的选择，下面对这两种方法分别介绍。
>
> 第一个方法是**在交叉验证中执行数据准备**：
>
> 在准备数据过程中，非常容易造成数据泄露，这将会过度拟合训练数据并且模型在未见过的数据上有着“过分乐观”的评估。例如，如果对整个数据集进行了归一化或标准化了，然后使用交叉验证来估计模型的性能，那么你就犯了数据泄露的错误。
> 这是因为在计算缩放因子(如最小和最大或平均值和标准差)时，执行的数据缩放过程是了解训练数据集中数据的完整分布的。这些知识被嵌入到重新缩放的值中，并被交叉验证测试工具中的所有算法利用。
> 在“无泄露评估”情况下，算法将会计算在交叉验证的每个fold中缩放数据的参数，并在每个周期中使用这些参数准备测试K-fold上的数据。
> 
> **注意：在准备数据、清理数据、填充缺失值、移除异常值等任何时候，都有可能产生数据泄漏。在准备数据的过程中，你可能会扭曲数据，以便在“干净”的数据集上构建一个运行良好的模型，但在你真正想要应用它的实际情况下，它将完全失败**。
> 
> 总的来说，在交叉验证中去重新准备和计算需要的数据当执行一些任务时，比如：特征选择、异常值删除、编码、特征缩放和降维的投影操作等，是一个非常好的办法。
> 
> 第二个方法是**保留验证数据集**：
> 
> 将数据集分为训练集、测试集和验证集。当完成最终的模型后，在验证集上进行测试。这可以给你一个全面的检查，看看你对性能的估计是否过于乐观并发生了泄漏。
> 本质上，真正解决这个问题的唯一方法是保留一个独立的测试集，并保持它直到研究完成，并使用它进行最终验证。
> 
> 其他的一些小技巧：
> 
> - 增加噪声：增加随机噪声可以平滑数据泄露的影响。
> - 使用Pipelines：使用Pipelines可以使得数据准备的过程可以被放入交叉验证中完美的执行，例如python中的scikit-learn。

### 3.1 留出法(Hold-out)

留出法简单来说就是将数据集划分成**训练集**、**测试集**和**验证集**。留出法有两个作用：第一是用作模型评估；第二是用作模型选择。

模型评估的用途其实很好理解，通过分割出训练集和测试集，使用训练集测试模型性能。而模型选择是将多个在此数据集中训练的不同的模型在验证集上进行验证，选出最优模型。

下文将先说明模型评估，随后再说明模型选择。

#### 3.1.1 模型评估

简单来说，留出法将数据集划分为用于模型训练的训练集和找到最优模型的测试集。这种方法通常使用在当**数据集比较小**的情况下并且没有足够的数据分为训练集、测试集和验证集。这种方法的优点在于实现简单。但是，对**数据集的划分**非常敏感，如果划分不是随机的，则会产生**偏差(bias)**，下图是对该方法的示意图。

> 关于偏差(Bias)和方差(Variance)
> 
> 理解不同的误差来源是如何导致偏差和方差可以帮助我们提升模型的准确度。我将会从概念、图像和数据角度分别介绍偏差和方差的定义。
> 
> 概念角度：
> 
> - 偏差：偏差通常被认为是真实值与预测值之间的差值。虽然只有一个模型的情况下讨论模型间的真实值与预测值的差值有点奇怪，但是假设**多次重复**整个模型的构建过程：每次运行新数据并运行新分析，创建新模型。由于基础数据的随机性，结果模型将会有一系列的预测。偏差衡量的是这些模型的**预测与正确值之间的总体偏差**。
> - 方差：方差引起的误差被认为是模型对给定数据点预测的可变性。同样，假设您可以多次重复整个模型构建过程。方差是指在不同的模型实现之间，**对给定点的预测有多大的差异**。
>
> 图像角度：
> 
> 如下图所示，中间红色的靶心代表真实值。如果点的分布距离靶心越远则代表偏差越大。如果点之间的分布越分散，则代表方差越大。
>
> ![](../../../pics/pics1/374.png)
> 
> - 左上角的示例是理想状况：偏差和方差都非常小。如果有无穷的训练数据，以及完美的模型算法，我们是有办法达成这样的情况的。然而，现实中的工程问题，通常数据量是有限的，而模型也是不完美的。因此，这只是一个理想状况。
> - 右上角的示例表示偏差小而方差大。靶纸上的落点都集中分布在红心周围，它们的期望落在红心之内，因此偏差较小。另外一方面，落点虽然集中在红心周围，但是比较分散，这是方差大的表现。
> - 左下角的示例表示偏差大二方差小。显而易见，靶纸上的落点非常集中，说明方差小。但是落点集中的位置距离红心很远，这是偏差大的表现。
> - 右下角的示例则是最糟糕的情况，偏差和方差都非常大。这是我们最不希望看到的结果。
>
> 数学角度：
>
> 假如我们定义预测值 $Y$ 和变量 $X$ ，我们可以假设两者之间的关系是 $Y=f(X)+\epsilon$ ，其中，$\epsilon$ 是误差项，通常是由均值为 $0$ 的正态分布表示： $\epsilon \sim N(0, \sigma_\epsilon)$ 。我们可以评估预测在线性模型 $\hat{f}(X)$ 与真实值 $f(X)$ 在预测点 $x$ 的误差为：
> 
> $$
> Err(x) = E[(Y-\hat{f}(x))^2]
> $$
> 
> 这个误差可以分解为偏差分量和方差分量:
>
> $$
> \begin{align}
> Err(x) &= [E[\hat{f}(x)] - f(x)]^2 + E[(\hat{f}(x) - E[\hat{f}(x)])^2] + \sigma^2_e \\
> Err(x) &= Bias^2 + Variance + Irreducible Error
> \end{align}
> $$
> 
> 其中，偏差描述的是通过学习拟合出来的结果之期望，与真实值之间的**差距**，记作 $Bias(X) = E[\hat{f}(X)] - f(X)$ ， 方差是统计学中的定义，描述的是通过学习拟合出来的结果自身的**不稳定性**， 记作 $Var(X) = E[(\hat{f}(x) - E[\hat{f}(x)])^2]$ ， Irreducible Error是**噪声**，任何模型都无法从根本上减少它。
> 
> 使用一个案例理解偏差与方差之间的关系。下图分别是使用线性模型和多项式模型对**训练集**数据的拟合。通过下图可知，在**训练集**上线性模型的误差(Linear Model Error: 3.6043)是明显高于多项式模型(Polynomial Model Error：1.6956)的：数据是围绕一个近似线性的函数附近抖动的，那么用简单的线性模型，自然就无法准确地拟合数据；但是，高阶的多项式函数可以进行各种“扭曲”，以便将训练集的数据拟合得更好。
> 
> ![](../../../pics/pics1/375.png)
> 
> 这种情况，我们说线性模型在训练集上**欠拟合(underfitting)**，并且它的偏差(bias)要高于多项式模型的偏差。
> 但这并不意味着在这个问题里线性模型要弱于多项式模型。在下图的**测试集**上，线性模型的误差(Linear Model Error:3.5998)要明显小于多项式模型的误差(Polynomial Model Error：929.1222)，并且线性模型在训练集和测试集的误差相对接近，而多项式模型在两个数据集上的误差相对较大。
> 这种情况被称为多项式模型在训练集上**过拟合(overfitting)**。由于线性模型在两个数据集上的误差较为接近，我们可以说线性模型在未见的数据集上，**泛化能力**更高。毕竟，在后续的工作中，会使用有限的训练集去拟合无限的未见的真是样本。所以，泛化能力是一个非常重要的指标。所以，我们说在这个模型上，线性模型比多项式模型性能更好。
> 
> ![](../../../pics/pics1/376.png)
> 
> 根据以上的实验，我们可以做出偏差-方差权衡(bias-variance tradeoff)：
> 
> 通过上面两张图的比较可以发现，模型的复杂程度（简单的线性模型和复杂的多项式模型）会影响最终的偏差和方差。具体来说，随着模型复杂程度的增加，其表述能力也增加，此时，模型在数据上的表现是偏差会降低而方差会升高；而随着模型复杂程度的降低，其表述能力会下降，此时，模型在数据上的表现是偏差会升高而方差会降低。带入误差公式我们可以绘制出下图：
> 
> ![](../../../pics/pics1/377.png)
> 
> 图中的最优点是Total Error曲线的最小值。根据一阶导数的性质可得，当 $Err\prime(x)=0$ 时，可得到误差的最小值，所以我们可得：
> 
> $$
> \frac{\partial {Bias}}{\partial {Complexity}} = - \frac{\partial {Varance}}{\partial {Complexity}}
> $$
> 
> 上述公式给出了最优点的数学描述，结合图像可知，当模型复杂度**大于**平衡点，则模型的方差会偏高，模型倾向于**过拟合**；而当模型复杂度**小于**平衡点，则模型的偏差会偏高，模型倾向于**欠拟合**。
> 所以，根据上文所述，将误差降低到0是无法实现的，尽管得到一个完美的模型使得训练集上的误差为0，但是还有噪音带来的影响和训练集本身存在的误差也会带入到模型中。其次，由于训练样本的是无法完美反应真实情况的（样本容量有限、采样不均匀等），以及模型学习本身存在上限，也就意味着我们不可能会有“完美模型”。因此，我们不会去训练误差为0的模型，而是使得模型的结果逼近最优结果。

![](../../../pics/pics1/373.png)

这个技术非常**容易产生过拟合**，因此，测试误差就变成了泛化误差的一个乐观偏差估计。然而，这并不是我们想要的。最终模型不能很好地泛化到未见过的或未来的数据集，[代码](../../../code/4/4.1/evaluation.ipynb)如下所示：

``` python
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)
lg = linear_model.LogisticRegression()
lg.fit(X_train, y_train)
y_pred = lg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
```

**注意，此过程用于模型评估，其基础是将数据集划分为训练数据集和测试数据集，并使用固定的超参数集**。还有一种技术是将数据分成三个集合（训练集、测试及和验证集），并使用这三个集合进行模型选择或超参数调优。我们将在下节介绍。

> 关于过拟合和欠拟合的说明。
> 
> 在实际环境中计算偏差和方差是比较困难的，所以，我们需要通过外在表现判断模型是否过拟合或者欠拟合。
> 在有限的数据集中，不断的增加模型的复杂度会使得误差不断降低，如下图：
> 
> ![](../../../pics/pics1/378.png)
> 
> 从上图可得：
> 
> - 模型复杂度低时，模型处于欠拟合状态，训练集和验证集的误差都很高。
> - 模型复杂度高时，模型处于过拟合状态，训练集误差较低而验证集误差较高。
> 
> 那么，有了这些分析，我们就可以判断出模型所处的拟合状态。根据Andew Ng提供的一般处理方法如下图：
> 
> ![](../../../pics/pics1/379.png)
> 
> - 当模型处于欠拟合状态时，根本的办法是增加模型复杂度。有以下方法：
>   - 增加模型的迭代次数；
>   - 更换描述能力更强的模型；
>   - 生成更多特征供训练使用；
>   - 降低正则化水平。
> - 当模型处于过拟合状态时，根本的办法是降低模型复杂度。有以下方法：
>   - 扩增训练集；
>   - 减少训练使用的特征的数量；
>   - 提高正则化水平。

#### 3.1.2 模型选择

保留方法也可以用于模型选择或超参数调优。事实上，有时模型选择过程也被称为超参数调优。在模型选择的留出法中，数据集被分为三个不同的集——训练集、验证集和测试集，如下图。在使用将数据划分为三个不同集的留出法时，重要的是要确保训练数据集、验证数据集和测试数据集能够代表整个数据集。否则，模型可能在不可见的数据上表现不佳。

![](../../../pics/pics1/380.png)

模型选择将会根据以下步骤选出最优模型：

1. 将数据集分割为**训练集**、**测试集**和**验证集**。
2. 使用不同的算法在**训练集**上训练模型，例如归回模型、随机森林或者是SVM等。
3. 对于用不同算法训练的模型，**调整超参数**，得到不同的模型。对于步骤2中提到的每个算法，更改超参数设置并提供多个模型。
4. 在**验证集**上测试每个模型(属于每个算法)的性能。
5. 从**验证集**上测试的模型中选择最优的模型。对于特定的算法，最优的模型将具有最优的超参数设置。
6. 在**测试集**上测试最优模型的性能。

上述过程如下图所示：

![](../../../pics/pics1/381.png)

**注意：原始数据集的三种不同划分。训练、调优和评估的过程重复多次，选择最优的模型。最终的模型在测试集上进行评估**。

[代码](../../../code/4/4.1/evaluation.ipynb)如下所示：

``` python
lg = linear_model.LogisticRegression()
svc = svm.SVC()
knn = neighbors.KNeighborsClassifier()
models = [lg.fit(X_train, y_train), svc.fit(X_train, y_train), knn.fit(X_train, y_train)]
best_acc = 0.0
best_model = lg
for model in models:
    y_pred = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    if best_acc > acc:
        best_model = model
        best_acc = acc
y_pred = best_model.predict(X_test)
print(accuracy_score(y_test, y_pred))
```

其结果是：

``` shell
LogisticRegression() , accracy:  0.9666666666666667
SVC() , accracy:  0.9333333333333333
KNeighborsClassifier() , accracy:  0.9333333333333333
1.0
```

这里会发现 `accuracy_score(y_test, y_pred)` 输出结果为1.0，其分数太高，怀疑过拟合。下文中的交叉验证法会重新评估模型。

### 3.2 交叉验证法(Cross Validation)

在上述的留出法中，存在一个缺陷：如果分割的数据集恰好对这个模型非常有利或者是分割恰好导致较低的偏差，这就导致泛化能力的降低。交叉验证时一种创造性地划分数据进行模型验证的方法，为了获得“真实世界”数据的模型性能的最好估计，同时最小化验证错误。

#### 3.2.1 什么是交叉验证法？

K折交叉验证被定义为一种估计在未见数据上模型性能的方法。当**数据稀缺**，并且需要对训练和**泛化误差**进行很好的估计时，建议使用这种技术，从而理解欠拟合和过拟合等方面的问题。这种技术用于**超参数调优**，以便可以训练具有最优超参数值的模型。这是一种没有替换的重采样技术。这种方法的优点是每个样本只用于一次训练和验证(作为测试部分)。这产生了比留出法**更低的模型性能方差估计**。如前所述，使用这种技术是因为它有助于**避免**使用所有数据训练模型时可能出现的**过拟合**。通过使用K折交叉验证，我们能够在K个不同的数据集上“测试”模型，这有助于确保模型是**可泛化**的。下面是当 $K=10$ 时，交叉验证的示意图（第二步至第七步）以及过程：

![](../../../pics/pics1/382.webp)

1. 将数据集划分为训练集和测试集。
2. 将训练集划分为K份。其中 $K$ 份为训练集
3. 将其中的 $K-1$ 份用于训练。
4. $1$ 份用于验证。 
5. 将带有不同超参数的模型放在 $K-1$ 份训练集中训练并且在剩下的一份验证集中验证。同时，记录模型的性能（第3、4、5步一直重复，直到经历 $K$ 次验证，这就是为什么被称作K折交叉验证）。
6. 取步骤5中计算的所有模型得分，计算模型性能的均值和标准差（如果只是评估模型，在这一步就已经得到K折后的平均分数）。 
7. 选择使得模型得分均值和标准差最优的超参数（如果进行模型选择，需要重复这一步）。
8. 最后，使用整个训练数据集训练模型(步骤2)，并在测试数据集上计算模型性能(步骤1)。

关于如何使用K-Fold进行模型评估，[代码](../../../code/4/4.1/evaluation.ipynb)如下所示：

``` python
pipeline = make_pipeline(StandardScaler(), linear_model.LogisticRegression())
strtfdKFold = StratifiedKFold(n_splits=10)
kfold = strtfdKFold.split(X, y)
scores = []
for k, (train, test) in enumerate(kfold):
    pipeline.fit(X[train,:], y[train])
    score = pipeline.score(X[test, :], y[test])
    scores.append(score)
    print('Fold: %2d, Training/Test Split Distribution: %s, Accuracy: %.3f' % (k+1, np.bincount(y[train]), score))
print('\n\nCross-Validation accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))
```

其结果是：

``` shell
Fold:  1, Training/Test Split Distribution: [45 45 45], Accuracy: 1.000
Fold:  2, Training/Test Split Distribution: [45 45 45], Accuracy: 0.933
Fold:  3, Training/Test Split Distribution: [45 45 45], Accuracy: 1.000
Fold:  4, Training/Test Split Distribution: [45 45 45], Accuracy: 1.000
Fold:  5, Training/Test Split Distribution: [45 45 45], Accuracy: 0.933
Fold:  6, Training/Test Split Distribution: [45 45 45], Accuracy: 0.933
Fold:  7, Training/Test Split Distribution: [45 45 45], Accuracy: 0.800
Fold:  8, Training/Test Split Distribution: [45 45 45], Accuracy: 1.000
Fold:  9, Training/Test Split Distribution: [45 45 45], Accuracy: 1.000
Fold: 10, Training/Test Split Distribution: [45 45 45], Accuracy: 1.000
Cross-Validation accuracy: 0.960 +/- 0.061
```

有个更简单的写法：

``` python
pipeline = make_pipeline(StandardScaler(), linear_model.LogisticRegression())
strtfdKFold = StratifiedKFold(n_splits=10)
scores = cross_val_score(pipeline, X, y, cv=10, n_jobs=1)
print('Cross Validation accuracy scores: %s' % scores)
print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))
```

其结果也是：

``` python
Cross Validation accuracy scores: [1.         0.93333333 1.         1.         0.93333333 0.93333333
 0.8        1.         1.         1.        ]
Cross Validation accuracy: 0.960 +/- 0.061
```

这里的结果是0.960 +/- 0.061，相比较于上文中 `accuracy_score(y_test, y_pred)` ，介个结果看起来合理一点（**这样判断并不科学，这里只是举个例子**）。

用一个例子进一步理解，假如我们有一个1000个样本的数据集，并且需要用k=5的交叉验证。首先，数据集按照 $8:2$ 的比例进行分割，即训练集为800，测试集为200。那么，在这800份训练集中中，需要按照k=5被分成5份（每份160个样本）。然后，可以在其中的4份中训练模型，在剩余的一份中验证性能。并且，将此过程重复五次，也就是说被分成的五份都可以被验证性能。其结果就是这个模型最终得到了五个估计矩阵（**然后得到的均值和标准差就是模型评估的分数**）。这个过程将会重复不同的超参数。然后，比较不同模型的均值和标准差，从所有模型中选择一个模型。最终，一个具有特定超参数和算法的新模型在整个800条记录上训练完成后在留出的200条记录上进行测试。

#### 3.2.2 为什么使用交叉验证法？

相比较留出法，交叉验证法有什么突出优势？在上文的留出法中，有两种划分，并带来不同的挑战，分别是：

- **训练集和测试集**：为了训练模型以获得最佳性能，需要适当调整超参数，以在测试数据上实现良好的模型性能。然而，这种技术存在过拟合测试集的风险。这是因为参数可以调整，直到estimator表现最佳。这样，关于测试集的知识可以“泄漏”到模型中，并且评估指标不再表现泛化性能。
- **训练集、验证集和测试集**：为了解决上述问题，我们创建了三个划分。它们是训练、验证和测试划分。使用训练和验证集对模型超参数进行调优。最后，使用测试数据来确定模型的泛化性能。然而，这种技术也有缺点。通过将数据划分为三个集合，可以减少用于模型学习的样本数量。结果取决于对(训练集、验证集)对的特定随机选择。

为了克服上述挑战，使用了**交叉验证技术**。创建了两种不同的划分，如训练集和测试集。然而，交叉验证是通过创建k折的训练数据来应用于训练数据，其中(K-1)折用于训练，其余折用于测试。该过程重复K次，并通过取所有创建的K个模型的均值和标准差来计算特定超参数集的模型性能。计算得到最优模型的超参数。最后，使用最优的超参数在训练数据集上再次训练模型，并通过计算模型在测试数据集上的性能来计算模型的泛化性能。

同时，交叉验证发也可以用于模型选择。下面总结了几个K折交叉验证的应用场景：

- **超参数调整**：可以用于估计不同超参数对于模型性能的影响，并且选出最佳超参数。
- **模型选择**：对于一些候选的模型，可以使用该技术去估计不同模型的性能。并且，选择最佳模型。
- **数据预处理**：可以用于估计不同数据预处理方案对于模型性能的影响。比如，使用不同的归一化方案对于模型性能的影响。

#### 3.2.3 K 值的选择

对于K值的选择有以下几个方法：

- K的标准值时10，并且可以用于适当的大小。
- 当数据集非常大时，K可以被设置为5。获得相对准确的估计的同时**减少计算成本**。
- 对于较少的数据集时，K可以适当增加。然而，较大的K值将会增加计算成本。由于数据集变少，因此该模型的性能估计方差会更高。
- 对于非常小的数据集，可以使用leave-one-out cross-validation (LOOCV)。在这种技术中，验证数据只包含一条记录。

**建议：使用StratifiedKFold，其目的是得到更佳的偏差和方差的估计，特别是在类别比例不均等的情况下**。

#### 3.2.4 交叉验证法的缺点

交叉验证法的缺点有**计算慢**和**难以并行化**。另外，K-Fold并不适合于所有的情况，例如以下场景：

- 不平衡数据集的分类问题：可以使用StratifiedKFold
- 不平衡数据集的回归问题。
- 非独立分布时，例如时间序列。

#### 3.2.5 总结

- K-Fold可用于模型选择和超参数选择
- k折交叉验证涉及将数据划分为训练数据集和测试数据集，在训练数据集上应用k折交叉验证，并选择性能最优的模型
- 有几种交叉验证生成器可以用于该技术，如KFold和StratifiedKFold。
- `Sklearn.model_selection` 模块的 `cross_val_score` 帮助类，可以使用一个简单的形式去使用K-Fold。 
- K的标准值时10，并且可以用于适当的大小。
- 当数据集非常大时，K可以被设置为5。获得相对准确的估计的同时**减少计算成本**。
- 对于较少的数据集时，K可以适当增加。然而，较大的K值将会增加计算成本。由于数据集变少，因此该模型的性能估计方差会更高。
- 对于非常小的数据集，可以使用leave-one-out cross-validation (LOOCV)。
- 建议使用StratifiedKFold，其目的是得到更佳的偏差和方差的估计，特别是在类别比例不均等的情况下。

### 3.3 自助法(Bootstrap)

设数据集 $D$ ，每次从数据集 $D$ 中进行**有放回取样**，挑选样本放入集合 $D\prime$ 中后再放回 $D$ 中，重复 $m$ 次，得到包含 $m$ 个样本的数据集。样本在m次采样中始终不被采到的概率是

$$
(1-\frac{1}{m})^m
$$

取极限得到

$$
\lim_{m \to \infty} = (1-\frac{1}{m})^m = \frac{1}{e} \approx 0.368
$$

即数据集 $D$ 约有36.8%的样本未出现在 $D \prime$ 中。于是将 $D \prime$ 作为训练集， $D \prime - D$ 作为测试集。这样，仍然使用 $m$ 个训练样本，但约有1/3未出现在训练集中的样本被用作测试集。

优点：自助法在数据集较小、难以有效划分训练/测试集时很有用；自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。
缺点：自助法改变了初始数据集的分布，这会引入估计偏差。

## 4. 回归问题常用的评估指标

### 4.1 平均绝对误差 (Mean Absolute Error, MAE)

### 4.2 平均绝对百分误差 (Mean Absolute Percentage Error, MAPE)

### 4.3 均方误差 (Mean Square Eroor, MSE)

### 4.4 均方根误差 (Root-Mean-Square Error, RMSE)

### 4.5 决定系数 ()

### 4.6 校正决定系数

### 4.7 回归问题评估指标适用场景

### 4.8 小结

## 5. 分类问题常用的评估指标

### 5.1 混淆矩阵 (Confusion Matrix)

什么是混淆矩阵？如下图所示，这是一个用于二分类的 $2 \times 2$ 混淆矩阵。

![](../../../pics/pics1/383.webp)

下面会以一个例子说明true negative, false negative, false positive, ture positive。

如果下图是一个预测病人中患有肿瘤的模型，测试数据集中包含100个患者。如下图所示：

![](../../../pics/pics1/384.webp)

- **True Positive(TP)**：模型正确地预测了正类(预测和实际值都是正类)。在上面的例子中，该模型对10名患有肿瘤的人进行了正向预测。
- **Ture Negative(TN)**：模型正确地预测了负类(预测和实际值都是负的)。在上面的例子中，模型对60名没有患肿瘤的人进行了负向预测。
- **False Positive(FP)**：模型错误地预测了负类(预测正例，实际负例)。在上面的例子中，有22人被预测为肿瘤阳性，尽管他们并没有肿瘤。FP也称为第一类错误(TYPE I error)。
- **Flase Negative(FN)**：模型错误地预测了正类(预测负类，实际正类)。在上面的例子中，有8名患有肿瘤的人被预测为阴性。FN也称为II型错误(TYPE II error)。

根据上面四个值，我们可以计算得出

- True Positive Rate: $TPR=\frac{TP}{Actual Positive}=\frac{TP}{TP+FN}$  
- False Negative Rate: $FPR=\frac{FN}{Actual Positive}=\frac{FN}{TP+FN}$
- True Negative Rate: $TNR=\frac{TN}{Actual Negative}=\frac{TN}{TN+FP}$
- False Negative Rate: $FNR=\frac{FP}{Actual Negative}=\frac{FP}{TN+FP}$

即使数据是不平衡的，我们也可以弄清楚我们的模型是否工作良好。因此，TPR和TNR应较高，FPR和FNR应尽可能低。有了这几个数据的帮助，其他的性能指标可以被计算。

### 5.2 精确率 (Accuracy)

准确性是最基本的评估指标，衡量的是**模型正确预测的百分比**。它的计算方法是用正确预测的次数除以模型预测的总次数。

$$
Accuracy=\frac{TP+TN}{TP+TN+FP+FN}
$$

但是，它有两个严重的问题，会导致指标失效：

- 对于有倾向性的问题，往往不能用精度指标来衡量：比如，判断空中的飞行物是导弹还是其他飞行物，很显然为了减少损失，我们更倾向于相信是导弹而采用相应的防护措施。此时判断为导弹实际上是其他飞行物与判断为其他飞行物实际上是导弹这两种情况的重要性是不一样的；
- 对于样本类别数量严重不均衡的情况，也不能用精度指标来衡量：比如银行客户样本中好客户990个，坏客户10个。如果一个模型直接把所有客户都判断为好客户，得到精度为99%，但这显然是没有意义的。

所以，对于以上两种情况，单纯使用Accuracy来衡量算法的优劣是无效的。因此，就需要对真实值和预测值做更深入的分析。

### 5.3 查准率 (Precision)

Precision衡量的是所有正例(TP+FP)预测中，真例(TP)预测所占的比例。它的计算方法是用真阳性数据的个数除以真阳性数据和假阳性数据的总和:

$$
Precision=\frac{TP}{TP+FP}
$$

当**假阳性的代价很高**时，Precision是一个很好的衡量标准。例如，垃圾邮件检测。在垃圾邮件检测中，假阳性意味着一封**非垃圾**邮件(实际为**阴性**)被识别为**垃圾**邮件(预测为垃圾邮件)。垃圾邮件检测模型的准确率不高，会导致用户丢失重要邮件。

### 5.4 查全率 (Recall)

召回率(Recall)，也称为灵敏度(sensitivity)，衡量的是数据集中所有实际正例样本中真实正例预测的比例。它的计算方法是，真阳性数据的个数除以真阳性数据和假阴性数据的和：

$$
Recall=\frac{TP}{TP+FN}
$$

应用同样的理解，我们知道，当存在与**假阴性相关的高成本**时，Recall应该是我们用来选择最佳模型的模型指标。例如，在欺诈检测或病人检测中。如果**欺诈性**交易(实际为**阳性**)被预测为**非欺诈性**交易(预测为**阴性**)，那么对银行来说，后果可能非常糟糕。同样，在病人检测中。如果一个生病的病人(实际呈**阳性**)通过了测试，但被预测为没有生病(预测为**阴性**)。如果该疾病具有传染性，假阴性的成本将非常高。

> 对于Pricision和Recall有以下几个场景可以辅助理解：
> 
> - 地震预测：对于地震的预测，我们希望的是Recall非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲Precision。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。“宁错拿一万，不放过一个”，分类阈值较低。
> - 嫌疑人定罪：基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。即使有时候放过了一些罪犯，但也是值得的。因此我们希望有较高的Precision值，可以合理地牺牲Recall。“宁放过一万，不错拿一个”，“疑罪从无”，分类阈值较高。
> 
> 问题1：某一家互联网金融公司风控部门的主要工作是利用机器模型抓取坏客户。互联网金融公司要扩大业务量，尽量多的吸引好客户，此时风控部门该怎样调整Recall和Precision？如果公司坏账扩大，公司缩紧业务，尽可能抓住更多的坏客户，此时风控部门该怎样调整Recall和Precision？
>
> 如果互联网公司要扩大业务量，为了减少好客户的误抓率，保证吸引更多的好客户，风控部门就会提高阈值，从而提高模型的查准率Precision，同时，导致查全率Recall下降。如果公司要缩紧业务，尽可能抓住更多的坏客户，风控部门就会降低阈值，从而提高模型的查全率Recall，但是这样会导致一部分好客户误抓，从而降低模型的查准率 Precision。

### 5.5 F1-Score 和 Fβ-Score 

根据上文的案例中，我们知道随着阈值的变化Recall和Precision往往会向着反方向变化，这种规律很难满足我们的期望，即Recall和Precision同时增大。有没有什么方法权衡Recall和Precision 的矛盾？

> 为什么准确率和召回率是互相影响？
> 
> recall和precision是相互矛盾的。如果想要更高的recall，那么就要让模型的预测能覆盖到更多的样本，但是这样模型就更有可能犯错，也就是说precision会比较低。如果模型很保守，只能检测出它很确定的样本，那么其precision会很高，但是recall会相对低。
>  
> - 当模型倾向于对样本进行较为保守的正类别预测时，只有非常确信的情况下才会将样本预测为正类别，这可能导致模型漏掉一些真正的正类别，从而使Recall降低。
> - 当模型更倾向于对样本进行激进的正类别预测时，即使对于不太确定的样本也会将其预测为正类别，这可能导致一些负类别样本被错误地预测为正类别，从而使Precision降低。

我们可以用一个指标来统一Recall和Precision的矛盾，即利用Recall和Precision的加权调和平均值作为衡量标准。

F-1 score是精确率和召回率的加权平均值，其中**权重是相等**的。它用于平衡精度和召回率之间的权衡：

$$
F1 score=\frac{2}{\frac{1}{Precision}+\frac{1}{Recall}}=2 \times \frac{Recall \times Precision}{Recall + Precision}
$$

F-1 score赋予召回率和准确率**相同的权重**。例如，如果一个模型具有高准确率但低召回率，这意味着它产生的假阳性较少，但错过了大量的真阳性。相比之下，具有高召回率但低准确率的模型产生了更多的假阳性，但捕获了更多的真阳性。在这种情况下，F-1 score可以帮助我们确定哪个模型更好。

有一个**加权**的F-1 score，我们可以给召回率和准确率赋予不同的权重。对于不同的问题对召回率和准确率的权重是不同的。所以，使用以下公式对不同方面进行加权：

$$
F_{\beta}=(1+\beta^2) \times \frac{Precision \times Recall}{\beta^2 \times Precision + Recall} 
$$

$\beta$ 表示召回率比准确率重要多少倍。如果召回率的重要性是准确率的两倍，则Beta值为2。

### 5.6 ROC曲线 (Receiver Operating Characteristic Curve)



### 5.7 AUC曲线 (Area Under ROC Curve)

### 5.8 PRC曲线 (Precision-Recall Curve)

### 5.10 分类问题评估指标适用场景

### 5.10 小结

## 6. 样本均衡与采样

## 7. 面试题