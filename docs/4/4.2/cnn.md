# 卷积神经网络

- 编辑：李竹楠
- 日期：2024/02/22

## 1. 从全连接到卷积

之前在[神经网络](./neural_network.md)中讨论的神经网络非常适合处理表格数据，其中行对应样本，列对应特征。对于表格数据，我们寻找的模式可能涉及**特征**之间的交互，但是我们不能预先假设任何与特征交互相关的**先验结构**。此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。例如：假设有一张 $100*100$ 的彩色图（一张很小的imgage），用一个vector去表示，那么它 有$100*100*3$ 个像素，也就是30000维，那么输入层就是30000维，假设hidden layer有1000个神经元，那么这个hidden layer的参数就是有 $30000*1000$。想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验。所以，CNN做的事就是简化神经网络的架构。

### 1.1 CNN如何工作

#### 1.1.1 不变性

想象一下，假设我们想从一张图片中找到某个物体。合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。理想情况下，我们的系统应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。但是，如果一只猪出现在图片顶部，我们还是应该认出它。我们可以从下图中得到灵感：在这个游戏中包含了许多充斥着活动的混乱场景，而沃尔多通常潜伏在一些不太可能的位置，读者的目标就是找出他。尽管沃尔多的装扮很有特点，但是在眼花缭乱的场景中找到他也如大海捞针。然而沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像。该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。卷积神经网络正是将 **空间不变性（spatial invariance）** 的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。

![](../../../pics/pics1/420.jpg)



#### 1.1.2 局部性



### 1.2 卷积

### 1.3 通道

## 2. CNN架构



### 2.1 卷积层

### 2.2 池化层

### 2.3 全连接层

## 面试题

### 深度学习为什么在计算机视觉领域这么好？

传统的计算机视觉方法需首先基于经验手动设计特征，然后使用分类器分类，这两个过程都是分开的。而深度学习里的卷积网络可实现对局部区域信息的提取，获得更高级的特征，当神经网络层数越多时，提取的特征会更抽象，将更有助于分类，同时神经网路将提取特征和分类融合在一个结构中。

### 阐述一下感受野的概念，并说一下在CNN中如何计算？

感受野指的是卷积神经网络每一层**输出**的特征图上每个像素点映射回**输入**图像上的区域的大小，神经元感受野的范围**越大**表示其接触到的原始图像范围就**越大**，也就意味着它能学习**更为全局**，语义层次更高的特征信息，相反，范围越小则表示其所包含的特征越趋向局部和细节。因此感受野的范围可以用来大致判断每一层的抽象层次，并且我们可以很明显地知道**网络越深**，神经元的**感受野越大**。卷积层的感受野大小与其之前层的**卷积核尺寸和步长**有关，与padding无关。

参考：
- https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807
- https://blog.csdn.net/weixin_41950276/article/details/82880465

### 上采样的原理和常用方式

在卷积神经网络中，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(如图像的语义分割)，这个使图像由小分辨率映射到大分辨率的操作，叫做上采样，它的实现一般有三种方式：

- 插值，一般使用的是双线性插值，因为效果最好，虽然计算上比其他插值方式复杂，但是相对于卷积计算可以说不值一提，其他插值方式还有最近邻插值、三线性插值等（插值所利用的信息越来越多，feature map越来越平滑，但是同时计算量也越来越大）；
- 转置卷积又或是说反卷积，双线性插值方法中不需要学习任何参数。而转置卷积就像卷积一样需要学习参数，通过对feature map的参数学习增加feature map的分辨率。
- Max Unpooling，在对称的max pooling位置记录最大值的索引位置，然后在unpooling阶段时将对应的值放置到原先最大值位置，其余位置补0。

参考：

- https://www.cnblogs.com/jiangkejie/p/12904304.html

### 下采样的作用是什么？通常有哪些方式？

下采样层有两个作用：

- 减少计算量，防止过拟合；
- 增大感受野，使得后面的卷积核能够学到更加全局的信息。

下采样的方式主要有两种：

- 采用stride为2的池化层，如Max-pooling和Average-pooling，目前通常使用Max-pooling，因为他**计算简单**而且能够更好的**保留纹理特征**；
- 采用stride为2的卷积层，下采样的过程是一个信息损失的过程，而池化层是不可学习的，用stride为2的可学习卷积层来代替pooling可以得到更好的效果，当然同时也增加了一定的计算量。

### 介绍一下空洞卷积的原理和作用

空洞卷积最初的提出是为了解决图像分割的问题而提出的,常见的图像分割算法通常使用**池化层和卷积层来增加感受野(Receptive Filed)**，同时也**缩小**了特征图尺寸(resolution)，然后再利用上采样还原图像尺寸，特征图缩小再放大的过程造成了**精度上的损失**，因此需要一种操作可以在**增加感受野的同时保持特征图的尺寸不变**，从而代替下采样和上采样操作，在这种需求下，空洞卷积就诞生了。

当然,如果不用空洞卷积这种方案,那怎么去弥补经过下采样而造成信息损失呢？其实，这是另一个思路了，于是才有了我们熟知的skip connection，它可以为上采样弥补信息，像FCN、U-Net这种典型的拓扑网络。其实我个人认为，如果一个问题如果从不同的思路去想的话，就会出现不同的解决方案。

与正常的卷积不同的是，空洞卷积引入了一个称为 **扩张率(dilation rate)** 的超参数，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。

缺点：

- 网格效应：连续使用三次r=2的空洞卷积会导致中间有很多空格，即很多像素没有利用到，这会导致出现网格效应，

如何解决？

既然空洞卷积存在着网格效应，那有什么方法可以解决这个问题吗。开门见山，论文中巧妙的使用了不同膨胀因子的空洞卷积，这样就能有效解决空洞卷积网格效应的问题。

### 深度可分离卷积的概念和作用

在计算资源受限制的移动端设备上，常规的卷积操作由于运算量大，经常难以满足实际运行速度的要求，这时，深度可分离卷积（Depthwise Separable Convolution）就排上了用场。深度可分离卷积是由**Depthwise(DW)卷积与Pointwise(PW)卷积**组成。该结构和常规卷积类似，可用来提取特征，但相比常规卷积，其参数量和运算成本较低。所以在一些轻量级网络中经常会用到此类结构，如MobileNet，ShuffleNet等。

参考：

- https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Separable_Convolution.html
- https://blog.csdn.net/kangdi7547/article/details/117925389

### 1×1卷积的概念和作用

- 降维，减少计算量；在ResNet模块中，先通过1×1卷积对通道数进行降通道，再送入3×3的卷积中，能够有效的减少神经网络的参数量和计算量；
- 升维；用最少的参数拓宽网络通道，通常在轻量级的网络中会用到，经过深度可分离卷积后，使用1×1卷积核增加通道的数量，例如mobilenet、shufflenet等；
- 实现跨通道的交互和信息整合；增强通道层面上特征融合的信息，在feature map尺度不变的情况下，实现通道升维、降维操作其实就是通道间信息的线性组合变化，也就是通道的信息交互整合的过程；
- 1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。

### 有哪些经典的卷积类型？

- 转置卷积：上采样
- 深度可分离卷积：使用Depthwise对每个输入特征分量与卷积核操作和使用1×1×channel的卷积核对feature mape做Pointwise操作（升维操作）。
- 1×1卷积：可以改变通道数量，从而达到降维升维的操作，并且可以添加非线性的特性在里面。
- 空洞卷积：增加感受野的同时保持特征图的尺寸不变。

参考：

- https://blog.csdn.net/weixin_37737254/article/details/102920408

### 有哪些增大感受野的方法？

空洞卷积、池化操作、较大卷积核尺寸的卷积操作。

### 神经网络中Addition / Concatenate区别是什么？

### 为什么神经网络种常用relu作为激活函数？

### 卷积层和全连接层的区别是什么？

卷积层是局部连接，所以提取的是局部信息；全连接层是全局连接，所以提取的是全局信息。

### ReLU函数在0处不可导，为什么还能用？

### Pooling层的作用以及如何进行反向传播

### 为什么max pooling 要更常用？什么场景下 average pooling 比 max pooling 更合适？

### CV中的卷积操作和数学上的严格定义的卷积的关系？

### 简述CNN分类网络的演变脉络及各自的贡献与特点